{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('word2vec').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"unlabeledTrainData.tsv\"):\n",
    "    with zipfile.ZipFile(\"unlabeledTrainData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "if not os.path.exists(\"labeledTrainData.tsv\"):\n",
    "    with zipfile.ZipFile(\"labeledTrainData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "if not os.path.exists(\"testData.tsv\"):\n",
    "    with zipfile.ZipFile(\"testData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|     id|              review|\n",
      "+-------+--------------------+\n",
      "| 9999_0|Watching Time Cha...|\n",
      "|45057_0|I saw this film a...|\n",
      "|15561_0|Minor Spoilers<br...|\n",
      "| 7161_0|I went to see thi...|\n",
      "|43971_0|Yes, I agree with...|\n",
      "|36495_0|Jennifer Ehle was...|\n",
      "|49472_0|Amy Poehler is a ...|\n",
      "|36693_0|A plane carrying ...|\n",
      "|  316_0|A well made, grit...|\n",
      "|32454_0|Incredibly dumb a...|\n",
      "|37128_0|After reading the...|\n",
      "|19439_0|It's hard to desc...|\n",
      "|10760_0|Of all the bile-i...|\n",
      "|15073_0|This is quite an ...|\n",
      "|33119_0|Being a huge Gary...|\n",
      "|38735_0|For the most part...|\n",
      "|12041_0|Ram Gopal Varma d...|\n",
      "|41565_0|I gave it 2 for s...|\n",
      "|48612_0|I wanted to watch...|\n",
      "|17525_0|Che is a good fil...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unsupervisedTrain = spark.read.csv('./unlabeledTrainData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "unsupervisedTrain.printSchema()\n",
    "unsupervisedTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    \n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class StripHtmlTags(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def strip_tags(html):\n",
    "            s = MLStripper()\n",
    "            s.feed(html)\n",
    "            return s.get_data()\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(strip_tags, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://stackoverflow.com/a/32337101/512251\n",
    "import nltk\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super().__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     id|              tokens|\n",
      "+-------+--------------------+\n",
      "| 9999_0|[Watching, Time, ...|\n",
      "|45057_0|[I, saw, this, fi...|\n",
      "|15561_0|[Minor, Spoilers,...|\n",
      "| 7161_0|[I, went, to, see...|\n",
      "|43971_0|[Yes, ,, I, agree...|\n",
      "|36495_0|[Jennifer, Ehle, ...|\n",
      "|49472_0|[Amy, Poehler, is...|\n",
      "|36693_0|[A, plane, carryi...|\n",
      "|  316_0|[A, well, made, ,...|\n",
      "|32454_0|[Incredibly, dumb...|\n",
      "|37128_0|[After, reading, ...|\n",
      "|19439_0|[It, ', s, hard, ...|\n",
      "|10760_0|[Of, all, the, bi...|\n",
      "|15073_0|[This, is, quite,...|\n",
      "|33119_0|[Being, a, huge, ...|\n",
      "|38735_0|[For, the, most, ...|\n",
      "|12041_0|[Ram, Gopal, Varm...|\n",
      "|41565_0|[I, gave, it, 2, ...|\n",
      "|48612_0|[I, wanted, to, w...|\n",
      "|17525_0|[Che, is, a, good...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "colName = 'review'\n",
    "\n",
    "stripper = StripHtmlTags(inputCol=colName, outputCol='strippedReview')\n",
    "tokenizer = NLTKWordPunctTokenizer(inputCol='strippedReview', outputCol='tokens')\n",
    "tokenizationPipeline = Pipeline(stages=[stripper, tokenizer])\n",
    "unsupervisedTrain = tokenizationPipeline.fit(unsupervisedTrain).transform(unsupervisedTrain)\n",
    "unsupervisedTrain = unsupervisedTrain.drop('review', 'strippedReview')\n",
    "unsupervisedTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|        word|              vector|\n",
      "+------------+--------------------+\n",
      "|      Talent|[0.05043964087963...|\n",
      "|       1910s|[0.00766492448747...|\n",
      "|   professed|[0.00649873120710...|\n",
      "|     Priests|[0.00435002613812...|\n",
      "|          CV|[-0.0500916466116...|\n",
      "|          Bu|[0.00334370625205...|\n",
      "|   Mikkelsen|[-0.0234730765223...|\n",
      "|     GÃ©gauff|[0.10754672437906...|\n",
      "|    quotient|[0.02462394163012...|\n",
      "|      Sadler|[0.10671693086624...|\n",
      "|    incident|[0.15025775134563...|\n",
      "|     misfire|[0.06377870589494...|\n",
      "|        buns|[0.04085354879498...|\n",
      "|precognition|[0.01148595195263...|\n",
      "|     serious|[0.23976345360279...|\n",
      "|       brink|[0.14629855751991...|\n",
      "|   showdowns|[0.03588028252124...|\n",
      "|       Milch|[0.06159516051411...|\n",
      "| ferociously|[0.01441349834203...|\n",
      "|     acronym|[0.07183517515659...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=200, seed=42, inputCol=\"tokens\", outputCol=\"wordVectors\")\n",
    "word2VecModel = word2Vec.fit(unsupervisedTrain)\n",
    "word2VecModel.getVectors().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+-------+---------+--------------------+\n",
      "|     id|sentiment|              review|\n",
      "+-------+---------+--------------------+\n",
      "| 5814_8|        1|With all this stu...|\n",
      "| 2381_9|        1|\"The Classic War ...|\n",
      "| 7759_3|        0|The film starts w...|\n",
      "| 3630_4|        0|It must be assume...|\n",
      "| 9495_8|        1|Superbly trashy a...|\n",
      "| 8196_8|        1|I dont know why p...|\n",
      "| 7166_2|        0|This movie could ...|\n",
      "|10633_1|        0|I watched this vi...|\n",
      "|  319_1|        0|A friend of mine ...|\n",
      "|8713_10|        1|<br /><br />This ...|\n",
      "| 2486_3|        0|What happens when...|\n",
      "|6811_10|        1|Although I genera...|\n",
      "|11744_9|        1|\"Mr. Harvey Light...|\n",
      "| 7369_1|        0|I had a feeling t...|\n",
      "|12081_1|        0|note to George Li...|\n",
      "| 3561_4|        0|Stephen King adap...|\n",
      "| 4489_1|        0|`The Matrix' was ...|\n",
      "| 3951_2|        0|Ulli Lommel's 198...|\n",
      "|3304_10|        1|This movie is one...|\n",
      "|9352_10|        1|Most people, espe...|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1|12500|\n",
      "|        0|12500|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervisedTrain = spark.read.csv('./labeledTrainData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "supervisedTrain.printSchema()\n",
    "supervisedTrain.show()\n",
    "supervisedTrain.groupby('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502578879999996"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='sentiment', featuresCol='wordVectors')\n",
    "classificationPipeline = Pipeline(stages=[tokenizationPipeline, word2VecModel, rf])\n",
    "grid = ParamGridBuilder().addGrid(rf.numTrees, [15, 20, 25, 30]).addGrid(rf.maxDepth, [3, 5]).build()\n",
    "cv = CrossValidator(estimator=classificationPipeline, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=BinaryClassificationEvaluator(labelCol='sentiment'),\n",
    "                    numFolds=10)\n",
    "\n",
    "model = cv.fit(supervisedTrain)\n",
    "supervisedTrain = model.transform(supervisedTrain)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='sentiment')\n",
    "r = evaluator.evaluate(supervisedTrain)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|      id|              review|\n",
      "+--------+--------------------+\n",
      "|12311_10|Naturally in a fi...|\n",
      "|  8348_2|This movie is a d...|\n",
      "|  5828_4|All in all, this ...|\n",
      "|  7186_2|Afraid of the Dar...|\n",
      "| 12128_7|A very accurate d...|\n",
      "|  2913_8|...as valuable as...|\n",
      "|  4396_1|This has to be on...|\n",
      "|   395_2|This is one of th...|\n",
      "| 10616_1|The worst movie i...|\n",
      "|  9074_9|Five medical stud...|\n",
      "|  9252_3|'The Mill on the ...|\n",
      "|  9896_9|I just saw this f...|\n",
      "|   574_4|\"The Love Letter\"...|\n",
      "| 11182_8|Another fantastic...|\n",
      "| 11656_4|This was included...|\n",
      "|  2322_4|I'm not really mu...|\n",
      "|  8703_1|This movie was dr...|\n",
      "|  7483_1|I don't think I'v...|\n",
      "| 6007_10|Excellent story-t...|\n",
      "| 12424_4|I completely forg...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('./testData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "test.printSchema()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      id|              review|      strippedReview|              tokens|         wordVectors|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|12311_10|Naturally in a fi...|Naturally in a fi...|[Naturally, in, a...|[0.03897609903848...|[5.74055357343557...|[0.22962214293742...|       1.0|\n",
      "|  8348_2|This movie is a d...|This movie is a d...|[This, movie, is,...|[0.03354414005419...|[17.6688367243038...|[0.70675346897215...|       0.0|\n",
      "|  5828_4|All in all, this ...|All in all, this ...|[All, in, all, ,,...|[0.03919219366508...|[12.9978412503189...|[0.51991365001275...|       0.0|\n",
      "|  7186_2|Afraid of the Dar...|Afraid of the Dar...|[Afraid, of, the,...|[0.02362638613847...|[12.2917913812105...|[0.49167165524842...|       1.0|\n",
      "| 12128_7|A very accurate d...|A very accurate d...|[A, very, accurat...|[0.04510661557527...|[9.68574218945903...|[0.38742968757836...|       1.0|\n",
      "|  2913_8|...as valuable as...|...as valuable as...|[..., as, valuabl...|[0.04024200208164...|[7.88255900749970...|[0.31530236029998...|       1.0|\n",
      "|  4396_1|This has to be on...|This has to be on...|[This, has, to, b...|[0.00673438862067...|[16.9076532981359...|[0.67630613192543...|       0.0|\n",
      "|   395_2|This is one of th...|This is one of th...|[This, is, one, o...|[0.02860345002382...|[13.2703811217069...|[0.53081524486827...|       0.0|\n",
      "| 10616_1|The worst movie i...|The worst movie i...|[The, worst, movi...|[0.00383784127975...|[17.3754791776786...|[0.69501916710714...|       0.0|\n",
      "|  9074_9|Five medical stud...|Five medical stud...|[Five, medical, s...|[0.04038100555979...|[8.14215268350707...|[0.32568610734028...|       1.0|\n",
      "|  9252_3|'The Mill on the ...|'The Mill on the ...|[', The, Mill, on...|[0.03662286974778...|[6.22983410140461...|[0.24919336405618...|       1.0|\n",
      "|  9896_9|I just saw this f...|I just saw this f...|[I, just, saw, th...|[0.04356995031444...|[11.8174254160031...|[0.47269701664012...|       1.0|\n",
      "|   574_4|\"The Love Letter\"...|\"The Love Letter\"...|[\", The, Love, Le...|[0.04361347796257...|[14.1738833011826...|[0.56695533204730...|       0.0|\n",
      "| 11182_8|Another fantastic...|Another fantastic...|[Another, fantast...|[0.05377046562909...|[12.2445145533693...|[0.48978058213477...|       1.0|\n",
      "| 11656_4|This was included...|This was included...|[This, was, inclu...|[0.03063480457485...|[13.6643256637789...|[0.54657302655115...|       0.0|\n",
      "|  2322_4|I'm not really mu...|I'm not really mu...|[I, ', m, not, re...|[0.04393269693310...|[13.0591769299654...|[0.52236707719861...|       0.0|\n",
      "|  8703_1|This movie was dr...|This movie was dr...|[This, movie, was...|[0.06356316102751...|[9.26622390311064...|[0.37064895612442...|       1.0|\n",
      "|  7483_1|I don't think I'v...|I don't think I'v...|[I, don, ', t, th...|[0.03884163077714...|[12.2608446191100...|[0.49043378476440...|       1.0|\n",
      "| 6007_10|Excellent story-t...|Excellent story-t...|[Excellent, story...|[0.03028946178579...|[8.25746013827923...|[0.33029840553116...|       1.0|\n",
      "| 12424_4|I completely forg...|I completely forg...|[I, completely, f...|[0.03743999666279...|[15.0570110694644...|[0.60228044277857...|       0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/jovyan/work/word2vec-nlp-tutorial/prediction.csv already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35180.csv.\n: org.apache.spark.sql.AnalysisException: path file:/home/jovyan/work/word2vec-nlp-tutorial/prediction.csv already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a3fa636b4c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    881\u001b[0m                        \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                        charToEscapeQuoteEscaping=charToEscapeQuoteEscaping)\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/jovyan/work/word2vec-nlp-tutorial/prediction.csv already exists.;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "test.select('id', 'prediction')\\\n",
    "    .coalesce(1)\\\n",
    "    .withColumn('sentiment', test['prediction'].cast(IntegerType()))\\\n",
    "    .drop('prediction')\\\n",
    "    .write.csv('prediction.csv', header='true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
