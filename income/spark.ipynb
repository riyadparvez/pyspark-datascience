{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('income').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: double (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: double (nullable = true)\n",
      " |-- capital-loss: double (nullable = true)\n",
      " |-- hours-per-week: double (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"age\", IntegerType(), True), \n",
    "#     StructField(\"workclass\", StringType(), True),\n",
    "#     StructField(\"fnlwgt\", FloatType(), True),\n",
    "#     StructField(\"education\", StringType(), True),\n",
    "#     StructField(\"education-num\", FloatType(), True),\n",
    "#     StructField(\"marital-status\", StringType(), True),\n",
    "#     StructField(\"occupation\", StringType(), True),\n",
    "#     StructField(\"relationship\", StringType(), True),\n",
    "#     StructField(\"race\", StringType(), True),\n",
    "#     StructField(\"sex\", StringType(), True),\n",
    "#     StructField(\"capital-gain\", FloatType(), True),\n",
    "#     StructField(\"capital-loss\", FloatType(), True),\n",
    "#     StructField(\"hours-per-week\", FloatType(), True),\n",
    "#     StructField(\"native-country\", StringType(), True),\n",
    "#     StructField(\"class\", StringType(), True)]\n",
    "# )\n",
    "\n",
    "# train = spark.read.csv('./adult.data.txt', schema=schema, inferSchema='true')\n",
    "\n",
    "headers = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n",
    "           \"class\"]\n",
    "\n",
    "train = spark.read.csv('./adult.data.txt', inferSchema='true').toDF(*headers)\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCol = 'class'\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|  >50K| 7841|\n",
      "| <=50K|24720|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def findMissingValuesCols(df):\n",
    "    numRows = df.count()\n",
    "    nullCols = []\n",
    "    for column in df.columns:\n",
    "        c = df.filter(col(column).isNotNull()).count()\n",
    "        if c != numRows:\n",
    "            nullCols.append(c)\n",
    "    return nullCols\n",
    "\n",
    "findMissingValuesCols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select('age').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "def crosstabPercentage(df, col1, col2):\n",
    "    ctabDf = df.crosstab(col1, col2)\n",
    "    ctabCol = col1 + '_' + col2\n",
    "    ctabNewCol = col1 + col2.title()\n",
    "    ctabDf = ctabDf.withColumn(ctabNewCol, ctabDf[ctabCol])\\\n",
    "                                         .orderBy(ctabNewCol).drop(ctabCol)\n",
    "    # Strip extra whitespaces from column name\n",
    "    for column in ctabDf.columns:\n",
    "        columnStripped = column.strip()\n",
    "        if column != columnStripped:\n",
    "            ctabDf = ctabDf.withColumn(column.strip(), ctabDf[column])\\\n",
    "                                         .drop(column)\n",
    "\n",
    "    ctabDf = ctabDf.withColumn('percentage-of->50K', \n",
    "                                                     format_number(ctabDf['>50K']/(ctabDf['<=50K']+ctabDf['>50K'])*100, 2))\n",
    "    return ctabDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'race', 'class')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'sex', 'class')\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'education', 'class')\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educationNumClass = crosstabPercentage(train, 'education-num', 'class')\n",
    "educationNumClass = educationNumClass.withColumn('education-numClassF', educationNumClass['education-numClass'].cast(DoubleType()))\\\n",
    "                                     .orderBy('education-numClassF').drop('education-numClass')\n",
    "cols = educationNumClass.columns\n",
    "cols.remove('education-numClassF')\n",
    "cols.insert(0, 'education-numClassF')\n",
    "educationNumClass = educationNumClass.select(cols)\n",
    "educationNumClass.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.crosstab('workclass', 'class').show()\n",
    "df = crosstabPercentage(train, 'workclass', 'class')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def autoIndexer(df, lableCol):\n",
    "    stringTypes = [dtype[0] for dtype in train.dtypes if dtype[1] == 'string']\n",
    "    indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "    try:\n",
    "        indexedTypes.remove(lableCol+'Indexed')\n",
    "    except:\n",
    "        pass\n",
    "    indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed') for stringType in stringTypes]\n",
    "    oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "    ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "    assembler = VectorAssembler(inputCols=oheTypes, outputCol='assembled')\n",
    "    pipeline = Pipeline(stages=[*indexers, ohe, assembler])\n",
    "#     assembler = VectorAssembler(inputCols=indexedTypes, outputCol='assembled')\n",
    "#     pipeline = Pipeline(stages=[*indexers, assembler])\n",
    "    \n",
    "    indexed = pipeline.fit(df).transform(df)\n",
    "    return stringTypes, oheTypes, indexed\n",
    "\n",
    "_, indexedTypes, indexedDf = autoIndexer(train, labelCol)\n",
    "# The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "# In general, small p-values (1% to 5%) would cause you to reject the null hypothesis. \n",
    "# This very large p-value (92.65%) means that the null hypothesis should not be rejected.\n",
    "testResult = ChiSquareTest.test(indexedDf, 'assembled', 'classIndexed')\n",
    "r = testResult.head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([(Vectors.dense(1.0, 0.0, 3.0), \n",
    "                             Vectors.dense(1.0, 0.0, 3.0), \n",
    "                             Vectors.dense(1.0, 0.0, 3.0))], \n",
    "                           [\"a\", \"b\", \"c\"])\n",
    "vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n",
    "df = vecAssembler.transform(df)\n",
    "df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stringTypes = [dtype[0] for dtype in train.dtypes if dtype[1] == 'string']\n",
    "indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed') for stringType in stringTypes]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "train = pipeline.fit(train).transform(train)\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "\n",
    "oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "assembler = VectorAssembler(inputCols=oheTypes, outputCol='assembled')\n",
    "pipeline = Pipeline(stages=[])\n",
    "train = ohe.fit(train).transform(train)\n",
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
