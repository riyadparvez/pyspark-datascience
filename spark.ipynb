{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|   x|prediction|\n",
      "+---+----+----------+\n",
      "|  1| 2.0|     false|\n",
      "|  2| 3.0|     false|\n",
      "|  3| 0.0|     false|\n",
      "|  4|99.0|      true|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'id'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (ChiSqSelector, HashingTF, \n",
    "Imputer, MinMaxScaler, Normalizer, QuantileDiscretizer, \n",
    "StandardScaler, Tokenizer)\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "\n",
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.sql.functions import avg, stddev_samp\n",
    "\n",
    "\n",
    "class HasMean(Params):\n",
    "\n",
    "    mean = Param(Params._dummy(), \"mean\", \"mean\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasMean, self).__init__()\n",
    "\n",
    "    def setMean(self, value):\n",
    "        return self._set(mean=value)\n",
    "\n",
    "    def getMean(self):\n",
    "        return self.getOrDefault(self.mean)\n",
    "    \n",
    "class HasStandardDeviation(Params):\n",
    "\n",
    "    stddev = Param(Params._dummy(), \"stddev\", \"stddev\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasStandardDeviation, self).__init__()\n",
    "\n",
    "    def setStddev(self, value):\n",
    "        return self._set(stddev=value)\n",
    "\n",
    "    def getStddev(self):\n",
    "        return self.getOrDefault(self.stddev)\n",
    "\n",
    "class HasCenteredThreshold(Params):\n",
    "\n",
    "    centered_threshold = Param(Params._dummy(),\n",
    "            \"centered_threshold\", \"centered_threshold\",\n",
    "            typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasCenteredThreshold, self).__init__()\n",
    "\n",
    "    def setCenteredThreshold(self, value):\n",
    "        return self._set(centered_threshold=value)\n",
    "\n",
    "    def getCenteredThreshold(self):\n",
    "        return self.getOrDefault(self.centered_threshold)\n",
    "\n",
    "    \n",
    "class NormalDeviation(Estimator, HasInputCol, \n",
    "        HasPredictionCol, HasCenteredThreshold):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        c = self.getInputCol()\n",
    "        mu, sigma = dataset.agg(avg(c), stddev_samp(c)).first()\n",
    "        return (NormalDeviationModel()\n",
    "            .setInputCol(c)\n",
    "            .setMean(mu)\n",
    "            .setStddev(sigma)\n",
    "            .setCenteredThreshold(self.getCenteredThreshold())\n",
    "            .setPredictionCol(self.getPredictionCol()))\n",
    "\n",
    "class NormalDeviationModel(Model, HasInputCol, HasPredictionCol,\n",
    "        HasMean, HasStandardDeviation, HasCenteredThreshold):\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getPredictionCol()\n",
    "        threshold = self.getCenteredThreshold()\n",
    "        mu = self.getMean()\n",
    "        sigma = self.getStddev()\n",
    "\n",
    "        return dataset.withColumn(y, (dataset[x] - mu) > threshold * sigma)\n",
    "    \n",
    "df = spark.createDataFrame([(1, 2.0), (2, 3.0), (3, 0.0), (4, 99.0)], [\"id\", \"x\"])\n",
    "\n",
    "normal_deviation = NormalDeviation().setInputCol(\"x\").setCenteredThreshold(1.0)\n",
    "model  = Pipeline(stages=[normal_deviation]).fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "df.show()\n",
    "f = df.schema.fields[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Param(parent='MinTransformation_48bf9ec018eaf3f54832', name='minimum', doc='minimum')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e9a04a7acbe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mminTransformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinTransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mconstTransformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstTransformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction_2\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0msetConst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminTransformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;31m# constTransformation = ConstTransformation().setInputCol(\"x\").setOutputCol(\"prediction\").setConst(20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-e9a04a7acbe0>\u001b[0m in \u001b[0;36mgetMin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetMin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m class MinTransformation(Estimator, HasInputCol, \n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetOrDefault\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultParamMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextractParamMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Param(parent='MinTransformation_48bf9ec018eaf3f54832', name='minimum', doc='minimum')"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (ChiSqSelector, HashingTF, \n",
    "Imputer, MinMaxScaler, Normalizer, QuantileDiscretizer, \n",
    "StandardScaler, Tokenizer)\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "\n",
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "class HasMin(Params):\n",
    "    minimum = Param(Params._dummy(), \"minimum\", \"minimum\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasMin, self).__init__()\n",
    "\n",
    "    def setMin(self, value):\n",
    "        return self._set(minimum=value)\n",
    "\n",
    "    def getMin(self):\n",
    "        return self.getOrDefault(self.minimum)\n",
    "\n",
    "class MinTransformation(Estimator, HasInputCol, \n",
    "        HasOutputCol, HasMin):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        c = self.getInputCol()\n",
    "        self.minimum = dataset.agg(min(c)).first()[0]\n",
    "        return (MinTransformationModel()\n",
    "            .setInputCol(c)\n",
    "            .setMin(self.minimum)\n",
    "            .setOutputCol(self.getOutputCol()))\n",
    "\n",
    "class MinTransformationModel(Model, HasInputCol, HasOutputCol, HasMin):\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getOutputCol()\n",
    "        minimum = self.getMin()\n",
    "        meta = { 'const' : minimum}\n",
    "        return dataset.withColumn(y, (col(x) - minimum).alias(y, metadata=meta))\n",
    "\n",
    "class HasConst(Params):\n",
    "    const = Param(Params._dummy(), \"const\", \"const\")\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasConst, self).__init__()\n",
    "\n",
    "    def setConst(self, value):\n",
    "        return self._set(const=value)\n",
    "\n",
    "    def getConst(self):\n",
    "        return self.getOrDefault(self.const)\n",
    "\n",
    "class ConstTransformation(Estimator, HasInputCol, \n",
    "        HasOutputCol, HasConst):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        c = self.getInputCol()\n",
    "        self.const = c.\n",
    "        return (ConstTransformationModel()\n",
    "            .setInputCol(c)\n",
    "            .setConst(const)\n",
    "            .setOutputCol(self.getOutputCol()))\n",
    "\n",
    "class ConstTransformationModel(Model, HasInputCol, HasOutputCol, HasConst):\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getOutputCol()\n",
    "        const = self.getConst()\n",
    "\n",
    "        return dataset.withColumn(y, col(x) + const)\n",
    "    \n",
    "df = spark.createDataFrame([(1, 2.0), (2, 3.0), (3, 1.0), (4, 99.0)], [\"id\", \"x\"])\n",
    "\n",
    "minTransformation = MinTransformation().setInputCol(\"x\").setOutputCol(\"prediction\")\n",
    "constTransformation = ConstTransformation().setInputCol(\"prediction\").setOutputCol(\"prediction_2\")\\\n",
    "            .setConst(minTransformation.getMin())\n",
    "# constTransformation = ConstTransformation().setInputCol(\"x\").setOutputCol(\"prediction\").setConst(20)\n",
    "\n",
    "# pipeline = Pipeline(stages=[minTransformation, constTransformation])\n",
    "# model  = pipeline.fit(df)\n",
    "\n",
    "model1 = minTransformation.fit(df)\n",
    "df = model1.transform(df)\n",
    "\n",
    "print(model1.getMin())\n",
    "\n",
    "model2 = constTransformation.fit(df)\n",
    "df = model2.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------------+\n",
      "|      features|standardScaledFeatures|minMaxScaledFeatures|\n",
      "+--------------+----------------------+--------------------+\n",
      "|[1.0,0.1,-1.0]|  [1.0,0.0181568259...|       [0.0,0.0,0.0]|\n",
      "| [2.0,1.1,1.0]|  [2.0,0.1997250857...|       [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]|  [3.0,1.8338394239...|       [1.0,1.0,1.0]|\n",
      "+--------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaledData = dataFrame\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"minMaxScaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(scaledData)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"standardScaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(scaledData)\n",
    "\n",
    "\n",
    "scaledData.select(\"features\", \"standardScaledFeatures\", \"minMaxScaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "# print(\"pValues: \" + str(r.pValues))\n",
    "# print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "# print(\"statistics: \" + str(r.statistics))\n",
    "\n",
    "df.agg(min('label')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = [(2, Vectors.dense(0.5, 10.0)),\n",
    "        (0, Vectors.dense(1.5, 20.0)),\n",
    "        (1, Vectors.dense(1.5, 30.0)),\n",
    "        (2, Vectors.dense(3.5, 30.0)),\n",
    "        (1, Vectors.dense(3.5, 40.0)),\n",
    "        (1, Vectors.dense(3.5, 40.0))]\n",
    "data = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "# train = splits[0]\n",
    "# test = splits[1]\n",
    "\n",
    "train = data\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [2, 5, 4, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
