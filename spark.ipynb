{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "def crosstabPercentage(df, col1, col2):\n",
    "    ctabDf = df.crosstab(col1, col2)\n",
    "    ctabCol = col1 + '_' + col2\n",
    "    ctabNewCol = col1 + col2.title()\n",
    "    ctabDf = ctabDf.withColumn(ctabNewCol, ctabDf[ctabCol])\\\n",
    "                                         .orderBy(ctabNewCol).drop(ctabCol)\n",
    "    # Strip extra whitespaces from column name\n",
    "    for column in ctabDf.columns:\n",
    "        columnStripped = column.strip()\n",
    "        if column != columnStripped:\n",
    "            ctabDf = ctabDf.withColumn(column.strip(), ctabDf[column])\\\n",
    "                                         .drop(column)\n",
    "\n",
    "#     ctabDf = ctabDf.withColumn('percentage-of->50K', ctabDf['>50K']/(ctabDf['<=50K']+ctabDf['>50K'])*100)\n",
    "#     return ctabDf\n",
    "    ctabDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x7f1e1ad7f8d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2, 3),\n",
    "    (2, 2, 3),\n",
    "    (4, 1, 3),\n",
    "    (4, 2, 3),\n",
    "], [\"label\", \"id2\", \"id3\"])\n",
    "labelCol = \"label\"\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w =  Window.partitionBy(df[labelCol]).orderBy(df['id2'])\n",
    "w\n",
    "# df.crosstab(\"id1\", \"id2\").show()\n",
    "# crosstabPercentage(df, \"id1\", \"id2\")\n",
    "\n",
    "# df2 = df.groupby(labelCol, \"id2\").count()\n",
    "# count = df.count()\n",
    "# df2.withColumn('percentage', col('count') / count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f8717479630>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby(\"id\")\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.687289278791,0.682270330336,0.0143058784354]\n",
      "degreesOfFreedom: [2, 3, 1]\n",
      "statistics: [0.75,1.5,6.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0, 10.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0, 20.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0, 10.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0, 10.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0, 20.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = [(2, Vectors.dense(0.5, 10.0)),\n",
    "        (0, Vectors.dense(1.5, 20.0)),\n",
    "        (1, Vectors.dense(1.5, 30.0)),\n",
    "        (2, Vectors.dense(3.5, 30.0)),\n",
    "        (1, Vectors.dense(3.5, 40.0)),\n",
    "        (1, Vectors.dense(3.5, 40.0))]\n",
    "data = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "# train = splits[0]\n",
    "# test = splits[1]\n",
    "\n",
    "train = data\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [2, 5, 4, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
