{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('income').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: double (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: double (nullable = true)\n",
      " |-- capital-loss: double (nullable = true)\n",
      " |-- hours-per-week: double (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"age\", IntegerType(), True), \n",
    "#     StructField(\"workclass\", StringType(), True),\n",
    "#     StructField(\"fnlwgt\", FloatType(), True),\n",
    "#     StructField(\"education\", StringType(), True),\n",
    "#     StructField(\"education-num\", FloatType(), True),\n",
    "#     StructField(\"marital-status\", StringType(), True),\n",
    "#     StructField(\"occupation\", StringType(), True),\n",
    "#     StructField(\"relationship\", StringType(), True),\n",
    "#     StructField(\"race\", StringType(), True),\n",
    "#     StructField(\"sex\", StringType(), True),\n",
    "#     StructField(\"capital-gain\", FloatType(), True),\n",
    "#     StructField(\"capital-loss\", FloatType(), True),\n",
    "#     StructField(\"hours-per-week\", FloatType(), True),\n",
    "#     StructField(\"native-country\", StringType(), True),\n",
    "#     StructField(\"class\", StringType(), True)]\n",
    "# )\n",
    "\n",
    "# train = spark.read.csv('./adult.data.txt', schema=schema, inferSchema='true')\n",
    "\n",
    "headers = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n",
    "           \"class\"]\n",
    "\n",
    "train = spark.read.csv('./adult.data.txt', inferSchema='true').toDF(*headers)\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "\n",
    "train = train.withColumn('index', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCol = 'class'\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|count|\n",
      "+------+-----+\n",
      "|  >50K| 7841|\n",
      "| <=50K|24720|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby(labelCol).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a class imbalance problem in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def findMissingValuesCols(df):\n",
    "    numRows = df.count()\n",
    "    nullCols = []\n",
    "    for column in df.columns:\n",
    "        c = df.filter(col(column).isNotNull()).count()\n",
    "        if c != numRows:\n",
    "            nullCols.append(c)\n",
    "    return nullCols\n",
    "\n",
    "findMissingValuesCols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 31|\n",
      "| 85|\n",
      "| 65|\n",
      "| 53|\n",
      "| 78|\n",
      "| 34|\n",
      "| 81|\n",
      "| 28|\n",
      "| 76|\n",
      "| 27|\n",
      "| 26|\n",
      "| 44|\n",
      "| 22|\n",
      "| 47|\n",
      "| 52|\n",
      "| 86|\n",
      "| 40|\n",
      "| 20|\n",
      "| 57|\n",
      "| 54|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('age').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "def crosstabPercentage(df, col1, col2):\n",
    "    ctabDf = df.crosstab(col1, col2)\n",
    "    ctabCol = col1 + '_' + col2\n",
    "    ctabNewCol = col1 + col2.title()\n",
    "    ctabDf = ctabDf.withColumn(ctabNewCol, ctabDf[ctabCol])\\\n",
    "                                         .orderBy(ctabNewCol).drop(ctabCol)\n",
    "    # Strip extra whitespaces from column name\n",
    "    for column in ctabDf.columns:\n",
    "        columnStripped = column.strip()\n",
    "        if column != columnStripped:\n",
    "            ctabDf = ctabDf.withColumn(column.strip(), ctabDf[column])\\\n",
    "                                         .drop(column)\n",
    "\n",
    "    ctabDf = ctabDf.withColumn('percentage-of->50K', ctabDf['>50K']/(ctabDf['<=50K']+ctabDf['>50K'])*100)\n",
    "    return ctabDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+------------------+\n",
      "|          raceClass|<=50K|>50K|percentage-of->50K|\n",
      "+-------------------+-----+----+------------------+\n",
      "|              Other|  246|  25|              9.23|\n",
      "| Amer-Indian-Eskimo|  275|  36|             11.58|\n",
      "|              Black| 2737| 387|             12.39|\n",
      "|              White|20699|7117|             25.59|\n",
      "| Asian-Pac-Islander|  763| 276|             26.56|\n",
      "+-------------------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crosstabPercentage(train, 'race', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+------------------+\n",
      "|ageClass|<=50K|>50K|percentage-of->50K|\n",
      "+--------+-----+----+------------------+\n",
      "|      17|  395|   0|              0.00|\n",
      "|      18|  550|   0|              0.00|\n",
      "|      19|  710|   2|              0.28|\n",
      "|      20|  753|   0|              0.00|\n",
      "|      21|  717|   3|              0.42|\n",
      "|      22|  752|  13|              1.70|\n",
      "|      23|  865|  12|              1.37|\n",
      "|      24|  767|  31|              3.88|\n",
      "|      25|  788|  53|              6.30|\n",
      "|      26|  722|  63|              8.03|\n",
      "|      27|  754|  81|              9.70|\n",
      "|      28|  748| 119|             13.73|\n",
      "|      29|  679| 134|             16.48|\n",
      "|      30|  690| 171|             19.86|\n",
      "|      31|  705| 183|             20.61|\n",
      "|      32|  639| 189|             22.83|\n",
      "|      33|  684| 191|             21.83|\n",
      "|      34|  643| 243|             27.43|\n",
      "|      35|  659| 217|             24.77|\n",
      "|      36|  635| 263|             29.29|\n",
      "|      37|  566| 292|             34.03|\n",
      "|      38|  545| 282|             34.10|\n",
      "|      39|  538| 278|             34.07|\n",
      "|      40|  526| 268|             33.75|\n",
      "|      41|  529| 279|             34.53|\n",
      "|      42|  510| 270|             34.62|\n",
      "|      43|  497| 273|             35.45|\n",
      "|      44|  443| 281|             38.81|\n",
      "|      45|  446| 288|             39.24|\n",
      "|      46|  445| 292|             39.62|\n",
      "|      47|  420| 288|             40.68|\n",
      "|      48|  326| 217|             39.96|\n",
      "|      49|  371| 206|             35.70|\n",
      "|      50|  341| 261|             43.36|\n",
      "|      51|  353| 242|             40.67|\n",
      "|      52|  286| 192|             40.17|\n",
      "|      53|  275| 189|             40.73|\n",
      "|      54|  242| 173|             41.69|\n",
      "|      55|  273| 146|             34.84|\n",
      "|      56|  248| 118|             32.24|\n",
      "|      57|  227| 131|             36.59|\n",
      "|      58|  244| 122|             33.33|\n",
      "|      59|  222| 133|             37.46|\n",
      "|      60|  211| 101|             32.37|\n",
      "|      61|  204|  96|             32.00|\n",
      "|      62|  191|  67|             25.97|\n",
      "|      63|  171|  59|             25.65|\n",
      "|      64|  155|  53|             25.48|\n",
      "|      65|  135|  43|             24.16|\n",
      "|      66|  115|  35|             23.33|\n",
      "|      67|  114|  37|             24.50|\n",
      "|      68|   93|  27|             22.50|\n",
      "|      69|   87|  21|             19.44|\n",
      "|      70|   70|  19|             21.35|\n",
      "|      71|   56|  16|             22.22|\n",
      "|      72|   58|   9|             13.43|\n",
      "|      73|   54|  10|             15.62|\n",
      "|      74|   39|  12|             23.53|\n",
      "|      75|   38|   7|             15.56|\n",
      "|      76|   41|   5|             10.87|\n",
      "|      77|   24|   5|             17.24|\n",
      "|      78|   18|   5|             21.74|\n",
      "|      79|   13|   9|             40.91|\n",
      "|      80|   20|   2|              9.09|\n",
      "|      81|   17|   3|             15.00|\n",
      "|      82|   12|   0|              0.00|\n",
      "|      83|    4|   2|             33.33|\n",
      "|      84|    9|   1|             10.00|\n",
      "|      85|    3|   0|              0.00|\n",
      "|      86|    1|   0|              0.00|\n",
      "|      87|    1|   0|              0.00|\n",
      "|      88|    3|   0|              0.00|\n",
      "|      90|   35|   8|             18.60|\n",
      "+--------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crosstabPercentage(train, 'age', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('ageClass').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+------------------+\n",
      "|sexClass|<=50K|>50K|percentage-of->50K|\n",
      "+--------+-----+----+------------------+\n",
      "|  Female| 9592|1179|             10.95|\n",
      "|    Male|15128|6662|             30.57|\n",
      "+--------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crosstabPercentage(train, 'sex', labelCol)\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+----+------------------+\n",
      "|educationClass|<=50K|>50K|percentage-of->50K|\n",
      "+--------------+-----+----+------------------+\n",
      "|     Preschool|   51|   0|              0.00|\n",
      "|       1st-4th|  162|   6|              3.57|\n",
      "|       5th-6th|  317|  16|              4.80|\n",
      "|          11th| 1115|  60|              5.11|\n",
      "|           9th|  487|  27|              5.25|\n",
      "|       7th-8th|  606|  40|              6.19|\n",
      "|          10th|  871|  62|              6.65|\n",
      "|          12th|  400|  33|              7.62|\n",
      "|       HS-grad| 8826|1675|             15.95|\n",
      "|  Some-college| 5904|1387|             19.02|\n",
      "|    Assoc-acdm|  802| 265|             24.84|\n",
      "|     Assoc-voc| 1021| 361|             26.12|\n",
      "|     Bachelors| 3134|2221|             41.48|\n",
      "|       Masters|  764| 959|             55.66|\n",
      "|   Prof-school|  153| 423|             73.44|\n",
      "|     Doctorate|  107| 306|             74.09|\n",
      "+--------------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = crosstabPercentage(train, 'education', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+------------------+\n",
      "|education-numClassF|<=50K|>50K|percentage-of->50K|\n",
      "+-------------------+-----+----+------------------+\n",
      "|                1.0|   51|   0|              0.00|\n",
      "|                2.0|  162|   6|              3.57|\n",
      "|                3.0|  317|  16|              4.80|\n",
      "|                4.0|  606|  40|              6.19|\n",
      "|                5.0|  487|  27|              5.25|\n",
      "|                6.0|  871|  62|              6.65|\n",
      "|                7.0| 1115|  60|              5.11|\n",
      "|                8.0|  400|  33|              7.62|\n",
      "|                9.0| 8826|1675|             15.95|\n",
      "|               10.0| 5904|1387|             19.02|\n",
      "|               11.0| 1021| 361|             26.12|\n",
      "|               12.0|  802| 265|             24.84|\n",
      "|               13.0| 3134|2221|             41.48|\n",
      "|               14.0|  764| 959|             55.66|\n",
      "|               15.0|  153| 423|             73.44|\n",
      "|               16.0|  107| 306|             74.09|\n",
      "+-------------------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "educationNumClass = crosstabPercentage(train, 'education-num', labelCol)\n",
    "educationNumClass = educationNumClass.withColumn('percentage-of->50K', \n",
    "                                    format_number(educationNumClass['percentage-of->50K'], 2))\n",
    "educationNumClass = educationNumClass.withColumn('education-numClassF', educationNumClass['education-numClass'].cast(DoubleType()))\\\n",
    "                                     .orderBy('education-numClassF').drop('education-numClass')\n",
    "cols = educationNumClass.columns\n",
    "cols.remove('education-numClassF')\n",
    "cols.insert(0, 'education-numClassF')\n",
    "educationNumClass = educationNumClass.select(cols)\n",
    "educationNumClass.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+-----+-----+--------+--------+--------+----+-----------+----------+----------+----------+--------+--------+----------+------------+-------------+\n",
      "|education-num_education| 10th| 11th| 12th| 1st-4th| 5th-6th| 7th-8th| 9th| Assoc-acdm| Assoc-voc| Bachelors| Doctorate| HS-grad| Masters| Preschool| Prof-school| Some-college|\n",
      "+-----------------------+-----+-----+-----+--------+--------+--------+----+-----------+----------+----------+----------+--------+--------+----------+------------+-------------+\n",
      "|                    5.0|    0|    0|    0|       0|       0|       0| 514|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                   10.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|         7291|\n",
      "|                   14.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|    1723|         0|           0|            0|\n",
      "|                    1.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|        51|           0|            0|\n",
      "|                    6.0|  933|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                    9.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|   10501|       0|         0|           0|            0|\n",
      "|                   13.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|      5355|         0|       0|       0|         0|           0|            0|\n",
      "|                    2.0|    0|    0|    0|     168|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                   12.0|    0|    0|    0|       0|       0|       0|   0|       1067|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                    7.0|    0| 1175|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                    3.0|    0|    0|    0|       0|     333|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                   16.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|       413|       0|       0|         0|           0|            0|\n",
      "|                   11.0|    0|    0|    0|       0|       0|       0|   0|          0|      1382|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                    8.0|    0|    0|  433|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                    4.0|    0|    0|    0|       0|       0|     646|   0|          0|         0|         0|         0|       0|       0|         0|           0|            0|\n",
      "|                   15.0|    0|    0|    0|       0|       0|       0|   0|          0|         0|         0|         0|       0|       0|         0|         576|            0|\n",
      "+-----------------------+-----+-----+-----+--------+--------+--------+----+-----------+----------+----------+----------+--------+--------+----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = train.crosstab('education-num', 'education')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that this is a sparse matrix, it's hard to find the non-zero values. So we will only focus on non-zero values to find out whether there is any relationship between these features and one of them is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"5.0_ 9th\": 514,\n",
      "    \"10.0_ Some-college\": 7291,\n",
      "    \"14.0_ Masters\": 1723,\n",
      "    \"1.0_ Preschool\": 51,\n",
      "    \"6.0_ 10th\": 933,\n",
      "    \"9.0_ HS-grad\": 10501,\n",
      "    \"13.0_ Bachelors\": 5355,\n",
      "    \"2.0_ 1st-4th\": 168,\n",
      "    \"12.0_ Assoc-acdm\": 1067,\n",
      "    \"7.0_ 11th\": 1175,\n",
      "    \"3.0_ 5th-6th\": 333,\n",
      "    \"16.0_ Doctorate\": 413,\n",
      "    \"11.0_ Assoc-voc\": 1382,\n",
      "    \"8.0_ 12th\": 433,\n",
      "    \"4.0_ 7th-8th\": 646,\n",
      "    \"15.0_ Prof-school\": 576\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, lit, when\n",
    "\n",
    "iterator = df.toLocalIterator()\n",
    "d = {}\n",
    "for row in iterator:\n",
    "    rowDict = row.asDict()\n",
    "    educationNum = rowDict['education-num_education']\n",
    "    for k, v in rowDict.items():\n",
    "        if k != 'education-num_education' and v != 0:\n",
    "            d[educationNum+'_'+k] = v\n",
    "\n",
    "import json\n",
    "s = json.dumps(d, indent=4)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it's obvious that these features are redundant. Only one of them should suffice for our classification task.\n",
    "\n",
    "Let's try more rigorous chi square test instead of something hand-wavy.\n",
    "\n",
    "First we will define an utility method that'll index the catgorical string columns, encodes them into one-hot-encoded vectors, and finally assemble all the feature vectos into once vector for later downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def autoIndexer(df, lableCol, outputCol='assembled'):\n",
    "    stringTypes = [dtype[0] for dtype in df.dtypes if dtype[1] == 'string']\n",
    "    indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "    try:\n",
    "        indexedTypes.remove(lableCol+'Indexed')\n",
    "    except:\n",
    "        pass\n",
    "    indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed') for stringType in stringTypes]\n",
    "    oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "    ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "    assembler = VectorAssembler(inputCols=oheTypes, outputCol=outputCol)\n",
    "    pipeline = Pipeline(stages=[*indexers, ohe, assembler])    \n",
    "    indexed = pipeline.fit(df).transform(df)\n",
    "    return stringTypes, oheTypes, indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexed = train.select('education-num', 'education')\n",
    "\n",
    "indexer = StringIndexer(inputCol='education', outputCol='educationIndexed')\n",
    "indexed = indexer.fit(indexed).transform(indexed)\n",
    "ohe = OneHotEncoderEstimator(inputCols=['education-num',], outputCols=['education-numOHE',])\n",
    "indexed = ohe.fit(indexed).transform(indexed)\n",
    "\n",
    "# The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "# In general, small p-values (1% to 5%) would cause you to reject the null hypothesis. \n",
    "# This very large p-value (92.65%) means that the null hypothesis should not be rejected.\n",
    "testResult = ChiSquareTest.test(indexed, 'education-numOHE', 'educationIndexed')\n",
    "r = testResult.head()\n",
    "print(\"pValues: \" + str(r.pValues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accept the hypothesis that features are dependent. We will drop the 'education' feature since the info. is covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'workclass', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'hours-per-week', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indexedTypes, indexedDf = autoIndexer(train, labelCol)\n",
    "# The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "# In general, small p-values (1% to 5%) would cause you to reject the null hypothesis. \n",
    "# This very large p-value (92.65%) means that the null hypothesis should not be rejected.\n",
    "testResult = ChiSquareTest.test(indexedDf, 'assembled', 'classIndexed')\n",
    "r = testResult.head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2, featuresCol='assembled')\n",
    "model = kmeans.fit(indexedDf)\n",
    "indexedDf = model.transform(indexedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDf.select('prediction', 'classIndexed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stringTypes = [dtype[0] for dtype in train.dtypes if dtype[1] == 'string']\n",
    "indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed') for stringType in stringTypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "\n",
    "# Fix columns\n",
    "oheTypes.remove('classIndexedOneHotEncoded')\n",
    "cols = train.columns[:]\n",
    "for oheType in oheTypes:\n",
    "    cols.append(oheType)\n",
    "for stringType in stringTypes:\n",
    "    cols.remove(stringType)\n",
    "\n",
    "cols.remove('index')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='assembled')\n",
    "classifier = GBTClassifier(featuresCol='assembled', labelCol='classIndexed')\n",
    "pipeline = Pipeline(stages=[*indexers, ohe, assembler, classifier])\n",
    "model = pipeline.fit(train)\n",
    "train = model.transform(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='classIndexed')\n",
    "metric = evaluator.evaluate(train)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.stages[-1]\n",
    "ohe = model.stages[-3]\n",
    "ohe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
