{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('word2vec').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"unlabeledTrainData.tsv\"):\n",
    "    with zipfile.ZipFile(\"unlabeledTrainData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "if not os.path.exists(\"labeledTrainData.tsv\"):\n",
    "    with zipfile.ZipFile(\"labeledTrainData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "if not os.path.exists(\"testData.tsv\"):\n",
    "    with zipfile.ZipFile(\"testData.tsv.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|     id|              review|\n",
      "+-------+--------------------+\n",
      "| 9999_0|Watching Time Cha...|\n",
      "|45057_0|I saw this film a...|\n",
      "|15561_0|Minor Spoilers<br...|\n",
      "| 7161_0|I went to see thi...|\n",
      "|43971_0|Yes, I agree with...|\n",
      "|36495_0|Jennifer Ehle was...|\n",
      "|49472_0|Amy Poehler is a ...|\n",
      "|36693_0|A plane carrying ...|\n",
      "|  316_0|A well made, grit...|\n",
      "|32454_0|Incredibly dumb a...|\n",
      "|37128_0|After reading the...|\n",
      "|19439_0|It's hard to desc...|\n",
      "|10760_0|Of all the bile-i...|\n",
      "|15073_0|This is quite an ...|\n",
      "|33119_0|Being a huge Gary...|\n",
      "|38735_0|For the most part...|\n",
      "|12041_0|Ram Gopal Varma d...|\n",
      "|41565_0|I gave it 2 for s...|\n",
      "|48612_0|I wanted to watch...|\n",
      "|17525_0|Che is a good fil...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unsupervisedTrain = spark.read.csv('./unlabeledTrainData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "unsupervisedTrain.printSchema()\n",
    "unsupervisedTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    \n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class StripHtmlTags(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def strip_tags(html):\n",
    "            s = MLStripper()\n",
    "            s.feed(html)\n",
    "            return s.get_data()\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(strip_tags, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://stackoverflow.com/a/32337101/512251\n",
    "import nltk\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super().__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     id|              tokens|\n",
      "+-------+--------------------+\n",
      "| 9999_0|[Watching, Time, ...|\n",
      "|45057_0|[I, saw, this, fi...|\n",
      "|15561_0|[Minor, Spoilers,...|\n",
      "| 7161_0|[I, went, to, see...|\n",
      "|43971_0|[Yes, ,, I, agree...|\n",
      "|36495_0|[Jennifer, Ehle, ...|\n",
      "|49472_0|[Amy, Poehler, is...|\n",
      "|36693_0|[A, plane, carryi...|\n",
      "|  316_0|[A, well, made, ,...|\n",
      "|32454_0|[Incredibly, dumb...|\n",
      "|37128_0|[After, reading, ...|\n",
      "|19439_0|[It, ', s, hard, ...|\n",
      "|10760_0|[Of, all, the, bi...|\n",
      "|15073_0|[This, is, quite,...|\n",
      "|33119_0|[Being, a, huge, ...|\n",
      "|38735_0|[For, the, most, ...|\n",
      "|12041_0|[Ram, Gopal, Varm...|\n",
      "|41565_0|[I, gave, it, 2, ...|\n",
      "|48612_0|[I, wanted, to, w...|\n",
      "|17525_0|[Che, is, a, good...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "colName = 'review'\n",
    "\n",
    "stripper = StripHtmlTags(inputCol=colName, outputCol='strippedReview')\n",
    "tokenizer = NLTKWordPunctTokenizer(inputCol='strippedReview', outputCol='tokens')\n",
    "tokenizationPipeline = Pipeline(stages=[stripper, tokenizer])\n",
    "unsupervisedTrain = tokenizationPipeline.fit(unsupervisedTrain).transform(unsupervisedTrain)\n",
    "unsupervisedTrain = unsupervisedTrain.drop('review', 'strippedReview')\n",
    "unsupervisedTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|        word|              vector|\n",
      "+------------+--------------------+\n",
      "|      Talent|[0.05043964087963...|\n",
      "|       1910s|[0.00766492448747...|\n",
      "|   professed|[0.00649873120710...|\n",
      "|     Priests|[0.00435002613812...|\n",
      "|          CV|[-0.0500916466116...|\n",
      "|          Bu|[0.00334370625205...|\n",
      "|   Mikkelsen|[-0.0234730765223...|\n",
      "|     GÃ©gauff|[0.10754672437906...|\n",
      "|    quotient|[0.02462394163012...|\n",
      "|      Sadler|[0.10671693086624...|\n",
      "|    incident|[0.15025775134563...|\n",
      "|     misfire|[0.06377870589494...|\n",
      "|        buns|[0.04085354879498...|\n",
      "|precognition|[0.01148595195263...|\n",
      "|     serious|[0.23976345360279...|\n",
      "|       brink|[0.14629855751991...|\n",
      "|   showdowns|[0.03588028252124...|\n",
      "|       Milch|[0.06159516051411...|\n",
      "| ferociously|[0.01441349834203...|\n",
      "|     acronym|[0.07183517515659...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=200, seed=42, inputCol=\"tokens\", outputCol=\"wordVectors\")\n",
    "word2VecModel = word2Vec.fit(unsupervisedTrain)\n",
    "word2VecModel.getVectors().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+-------+---------+--------------------+\n",
      "|     id|sentiment|              review|\n",
      "+-------+---------+--------------------+\n",
      "| 5814_8|        1|With all this stu...|\n",
      "| 2381_9|        1|\"The Classic War ...|\n",
      "| 7759_3|        0|The film starts w...|\n",
      "| 3630_4|        0|It must be assume...|\n",
      "| 9495_8|        1|Superbly trashy a...|\n",
      "| 8196_8|        1|I dont know why p...|\n",
      "| 7166_2|        0|This movie could ...|\n",
      "|10633_1|        0|I watched this vi...|\n",
      "|  319_1|        0|A friend of mine ...|\n",
      "|8713_10|        1|<br /><br />This ...|\n",
      "| 2486_3|        0|What happens when...|\n",
      "|6811_10|        1|Although I genera...|\n",
      "|11744_9|        1|\"Mr. Harvey Light...|\n",
      "| 7369_1|        0|I had a feeling t...|\n",
      "|12081_1|        0|note to George Li...|\n",
      "| 3561_4|        0|Stephen King adap...|\n",
      "| 4489_1|        0|`The Matrix' was ...|\n",
      "| 3951_2|        0|Ulli Lommel's 198...|\n",
      "|3304_10|        1|This movie is one...|\n",
      "|9352_10|        1|Most people, espe...|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1|12500|\n",
      "|        0|12500|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervisedTrain = spark.read.csv('./labeledTrainData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "supervisedTrain.printSchema()\n",
    "supervisedTrain.show()\n",
    "supervisedTrain.groupby('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444512255999989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='sentiment', featuresCol='wordVectors')\n",
    "classificationPipeline = Pipeline(stages=[tokenizationPipeline, word2VecModel, rf])\n",
    "model = classificationPipeline.fit(supervisedTrain)\n",
    "supervisedTrain = model.transform(supervisedTrain)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='sentiment')\n",
    "r = evaluator.evaluate(supervisedTrain)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|      id|              review|\n",
      "+--------+--------------------+\n",
      "|12311_10|Naturally in a fi...|\n",
      "|  8348_2|This movie is a d...|\n",
      "|  5828_4|All in all, this ...|\n",
      "|  7186_2|Afraid of the Dar...|\n",
      "| 12128_7|A very accurate d...|\n",
      "|  2913_8|...as valuable as...|\n",
      "|  4396_1|This has to be on...|\n",
      "|   395_2|This is one of th...|\n",
      "| 10616_1|The worst movie i...|\n",
      "|  9074_9|Five medical stud...|\n",
      "|  9252_3|'The Mill on the ...|\n",
      "|  9896_9|I just saw this f...|\n",
      "|   574_4|\"The Love Letter\"...|\n",
      "| 11182_8|Another fantastic...|\n",
      "| 11656_4|This was included...|\n",
      "|  2322_4|I'm not really mu...|\n",
      "|  8703_1|This movie was dr...|\n",
      "|  7483_1|I don't think I'v...|\n",
      "| 6007_10|Excellent story-t...|\n",
      "| 12424_4|I completely forg...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('./testData.tsv', sep='\\t', header='true', inferSchema='true')\n",
    "test.printSchema()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      id|              review|      strippedReview|              tokens|         wordVectors|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|12311_10|Naturally in a fi...|Naturally in a fi...|[Naturally, in, a...|[0.03897609903848...|[5.32248836958490...|[0.26612441847924...|       1.0|\n",
      "|  8348_2|This movie is a d...|This movie is a d...|[This, movie, is,...|[0.03354414005419...|[14.7326861095650...|[0.73663430547825...|       0.0|\n",
      "|  5828_4|All in all, this ...|All in all, this ...|[All, in, all, ,,...|[0.03919219366508...|[10.4178415483263...|[0.52089207741631...|       0.0|\n",
      "|  7186_2|Afraid of the Dar...|Afraid of the Dar...|[Afraid, of, the,...|[0.02362638613847...|[8.95594467385881...|[0.44779723369294...|       1.0|\n",
      "| 12128_7|A very accurate d...|A very accurate d...|[A, very, accurat...|[0.04510661557527...|[6.16138614454114...|[0.30806930722705...|       1.0|\n",
      "|  2913_8|...as valuable as...|...as valuable as...|[..., as, valuabl...|[0.04024200208164...|[7.56921408994548...|[0.37846070449727...|       1.0|\n",
      "|  4396_1|This has to be on...|This has to be on...|[This, has, to, b...|[0.00673438862067...|[14.5752174722903...|[0.72876087361451...|       0.0|\n",
      "|   395_2|This is one of th...|This is one of th...|[This, is, one, o...|[0.02860345002382...|[11.1785830494402...|[0.55892915247201...|       0.0|\n",
      "| 10616_1|The worst movie i...|The worst movie i...|[The, worst, movi...|[0.00383784127975...|[12.7334707971032...|[0.63667353985516...|       0.0|\n",
      "|  9074_9|Five medical stud...|Five medical stud...|[Five, medical, s...|[0.04038100555979...|[7.61714235008465...|[0.38085711750423...|       1.0|\n",
      "|  9252_3|'The Mill on the ...|'The Mill on the ...|[', The, Mill, on...|[0.03662286974778...|[4.08414999210830...|[0.20420749960541...|       1.0|\n",
      "|  9896_9|I just saw this f...|I just saw this f...|[I, just, saw, th...|[0.04356995031444...|[7.34847298306686...|[0.36742364915334...|       1.0|\n",
      "|   574_4|\"The Love Letter\"...|\"The Love Letter\"...|[\", The, Love, Le...|[0.04361347796257...|[10.5883821304623...|[0.52941910652311...|       0.0|\n",
      "| 11182_8|Another fantastic...|Another fantastic...|[Another, fantast...|[0.05377046562909...|[8.13019638489316...|[0.40650981924465...|       1.0|\n",
      "| 11656_4|This was included...|This was included...|[This, was, inclu...|[0.03063480457485...|[10.7818825617161...|[0.53909412808580...|       0.0|\n",
      "|  2322_4|I'm not really mu...|I'm not really mu...|[I, ', m, not, re...|[0.04393269693310...|[12.6040393159192...|[0.63020196579596...|       0.0|\n",
      "|  8703_1|This movie was dr...|This movie was dr...|[This, movie, was...|[0.06356316102751...|[7.52347093443293...|[0.37617354672164...|       1.0|\n",
      "|  7483_1|I don't think I'v...|I don't think I'v...|[I, don, ', t, th...|[0.03884163077714...|[12.0583702513495...|[0.60291851256747...|       0.0|\n",
      "| 6007_10|Excellent story-t...|Excellent story-t...|[Excellent, story...|[0.03028946178579...|[6.30071887195083...|[0.31503594359754...|       1.0|\n",
      "| 12424_4|I completely forg...|I completely forg...|[I, completely, f...|[0.03743999666279...|[11.4329635776427...|[0.57164817888213...|       0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "test.select('id', 'prediction')\\\n",
    "    .coalesce(1)\\\n",
    "    .withColumn('sentiment', test['prediction'].cast(IntegerType()))\\\n",
    "    .drop('prediction')\\\n",
    "    .write.csv('prediction.csv', header='true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
