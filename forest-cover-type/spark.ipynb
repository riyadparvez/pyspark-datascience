{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('covtype.data.gz'):\n",
    "    urllib.request.urlretrieve('http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz', 'covtype.data.gz')\n",
    "    with gzip.open('covtype.data.gz', 'rb') as f_in:\n",
    "        with open('covtype.data', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "if not os.path.exists('covtype.info'):\n",
    "    urllib.request.urlretrieve('http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info', 'covtype.info')\n",
    "if not os.path.exists('old_covtype.info'):\n",
    "    urllib.request.urlretrieve('http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/old_covtype.info', 'old_covtype.info')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"forest-cover-type\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://60335d42cda3:4040'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('covtype.data', inferSchema='true')\n",
    "# Wilderness_Area and Soil_Type columns are one hot encoded\n",
    "Wilderness_Area_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "Soil_Type_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "\n",
    "headers = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "          'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n",
    "          'Horizontal_Distance_To_Fire_Points', *Wilderness_Area_cols,\n",
    "           *Soil_Type_cols, 'Cover_Type']\n",
    "data = data.toDF(*headers)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Cover_Type: integer (nullable = true)\n",
      " |-- Wilderness_Area_features: vector (nullable = true)\n",
      " |-- Soil_Type_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "wildernessAssembler = VectorAssembler(inputCols=Wilderness_Area_cols, outputCol='Wilderness_Area_features')\n",
    "soilTypeAssembler = VectorAssembler(inputCols=Soil_Type_cols, outputCol='Soil_Type_features')\n",
    "pipeline = Pipeline(stages=[wildernessAssembler, soilTypeAssembler])\n",
    "data = pipeline.fit(data).transform(data)\n",
    "data = data.drop(*Wilderness_Area_cols, *Soil_Type_cols)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Cover_Type| count|\n",
      "+----------+------+\n",
      "|         1|170048|\n",
      "|         6| 13911|\n",
      "|         3| 28741|\n",
      "|         5|  7587|\n",
      "|         4|  2214|\n",
      "|         7| 16563|\n",
      "|         2|226654|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2])\n",
    "labelCol = 'Cover_Type'\n",
    "train.groupby(labelCol).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "cols = data.columns[:]\n",
    "cols.remove(labelCol)\n",
    "pcaAssembler = VectorAssembler(inputCols=cols, outputCol='pcaFeatures')\n",
    "pca = PCA(k=48, inputCol='pcaFeatures', outputCol='pcaOutFeatures')\n",
    "classifier = RandomForestClassifier(featuresCol='pcaOutFeatures', \n",
    "                                    labelCol=labelCol,\n",
    "                                    numTrees=50,\n",
    "                                    maxDepth=25)\n",
    "pcaPipeline = Pipeline(stages=[pcaAssembler, pca, classifier])\n",
    "pcaPipelineModel = pcaPipeline.fit(train)\n",
    "df = pcaPipelineModel.transform(train)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Cover_Type')\n",
    "evaluator.evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "cols = data.columns[:]\n",
    "cols.remove(labelCol)\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "classifier = RandomForestClassifier(featuresCol='features', \n",
    "                                    labelCol=labelCol,\n",
    "                                    numTrees=20,\n",
    "                                    maxDepth=25)\n",
    "pipeline = Pipeline(stages=[assembler, classifier])\n",
    "pipelineModel = pipeline.fit(train)\n",
    "df = pipelineModel.transform(train)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol)\n",
    "evaluator.evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('pcaOutFeatures').show()\n",
    "# df.describe('pcaOutFeatures').show()\n",
    "\n",
    "pcaModel = pcaPipelineModel.stages[-1]\n",
    "pcaModel.explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, max\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "def l2norm(v):\n",
    "    x = v.toArray()\n",
    "    d = np.linalg.norm(x)\n",
    "#     return float(d)\n",
    "    return d\n",
    "\n",
    "# l2norm(df.select('pcaOutFeatures').head()[0])\n",
    "df.select('pcaOutFeatures')\n",
    "l2_norm_udf = udf(lambda x: l2norm(x), DoubleType())\n",
    "dfDist = df.withColumn('dist', l2_norm_udf(df['pcaOutFeatures']))\n",
    "dfDist.agg(max(col('dist'))).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
