{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('income').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"age\", IntegerType(), True), \n",
    "#     StructField(\"workclass\", StringType(), True),\n",
    "#     StructField(\"fnlwgt\", FloatType(), True),\n",
    "#     StructField(\"education\", StringType(), True),\n",
    "#     StructField(\"education-num\", FloatType(), True),\n",
    "#     StructField(\"marital-status\", StringType(), True),\n",
    "#     StructField(\"occupation\", StringType(), True),\n",
    "#     StructField(\"relationship\", StringType(), True),\n",
    "#     StructField(\"race\", StringType(), True),\n",
    "#     StructField(\"sex\", StringType(), True),\n",
    "#     StructField(\"capital-gain\", FloatType(), True),\n",
    "#     StructField(\"capital-loss\", FloatType(), True),\n",
    "#     StructField(\"hours-per-week\", FloatType(), True),\n",
    "#     StructField(\"native-country\", StringType(), True),\n",
    "#     StructField(\"class\", StringType(), True)]\n",
    "# )\n",
    "\n",
    "# train = spark.read.csv('./adult.data.txt', schema=schema, inferSchema='true')\n",
    "\n",
    "headers = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n",
    "           \"class\"]\n",
    "\n",
    "train = spark.read.csv('./adult.data.txt',\n",
    "                       inferSchema='true', \n",
    "                       ignoreLeadingWhiteSpace='true',\n",
    "                       ignoreTrailingWhiteSpace='true').toDF(*headers)\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "\n",
    "train = train.withColumn('index', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCol = 'class'\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|<=50K|24720|\n",
      "| >50K| 7841|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby(labelCol).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a class imbalance problem in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def findMissingValuesCols(df):\n",
    "    numRows = df.count()\n",
    "    nullCols = []\n",
    "    for column in df.columns:\n",
    "        c = df.filter(col(column).isNotNull()).count()\n",
    "        if c != numRows:\n",
    "            nullCols.append(c)\n",
    "    return nullCols\n",
    "\n",
    "findMissingValuesCols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 31|\n",
      "| 85|\n",
      "| 65|\n",
      "| 53|\n",
      "| 78|\n",
      "| 34|\n",
      "| 81|\n",
      "| 28|\n",
      "| 76|\n",
      "| 27|\n",
      "| 26|\n",
      "| 44|\n",
      "| 22|\n",
      "| 47|\n",
      "| 52|\n",
      "| 86|\n",
      "| 40|\n",
      "| 20|\n",
      "| 57|\n",
      "| 54|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('age').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import format_number, lit\n",
    "\n",
    "def crosstabPercentage(df, col1, col2):\n",
    "    df2 = df.groupby(col1, col2).count()\n",
    "    count = df.count()\n",
    "    df2 = df2.withColumn('percentage', col('count') / count * 100)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+----------+\n",
      "|              race|class|count|percentage|\n",
      "+------------------+-----+-----+----------+\n",
      "|             Other| >50K|   25|      0.08|\n",
      "|Amer-Indian-Eskimo| >50K|   36|      0.11|\n",
      "|             Other|<=50K|  246|      0.76|\n",
      "|Amer-Indian-Eskimo|<=50K|  275|      0.84|\n",
      "|Asian-Pac-Islander| >50K|  276|      0.85|\n",
      "|             Black| >50K|  387|      1.19|\n",
      "|Asian-Pac-Islander|<=50K|  763|      2.34|\n",
      "|             Black|<=50K| 2737|      8.41|\n",
      "|             White| >50K| 7117|     21.86|\n",
      "|             White|<=50K|20699|     63.57|\n",
      "+------------------+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentageCol = 'percentage'\n",
    "df = crosstabPercentage(train, 'race', labelCol).orderBy(percentageCol)\n",
    "df = df.orderBy(percentageCol).withColumn(percentageCol, \n",
    "                                    format_number(df[percentageCol], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'age', labelCol).orderBy(percentageCol)\n",
    "df = df.orderBy('ageClass').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'sex', labelCol)\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'education', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educationNumClass = crosstabPercentage(train, 'education-num', labelCol)\n",
    "educationNumClass = educationNumClass.withColumn('percentage-of->50K', \n",
    "                                    format_number(educationNumClass['percentage-of->50K'], 2))\n",
    "educationNumClass = educationNumClass.withColumn('education-numClassF', educationNumClass['education-numClass'].cast(DoubleType()))\\\n",
    "                                     .orderBy('education-numClassF').drop('education-numClass')\n",
    "cols = educationNumClass.columns\n",
    "cols.remove('education-numClassF')\n",
    "cols.insert(0, 'education-numClassF')\n",
    "educationNumClass = educationNumClass.select(cols)\n",
    "educationNumClass.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.crosstab('education-num', 'education')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that this is a sparse matrix, it's hard to find the non-zero values. So we will only focus on non-zero values to find out whether there is any relationship between these features and one of them is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit, when\n",
    "\n",
    "iterator = df.toLocalIterator()\n",
    "d = {}\n",
    "for row in iterator:\n",
    "    rowDict = row.asDict()\n",
    "    educationNum = rowDict['education-num_education']\n",
    "    for k, v in rowDict.items():\n",
    "        if k != 'education-num_education' and v != 0:\n",
    "            d[educationNum+'_'+k] = v\n",
    "\n",
    "import json\n",
    "s = json.dumps(d, indent=4)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it's obvious that these features are redundant. Only one of them should suffice for our classification task.\n",
    "\n",
    "Let's try more rigorous chi square test instead of something hand-wavy.\n",
    "\n",
    "First we will define an utility method that'll index the catgorical string columns, encodes them into one-hot-encoded vectors, and finally assemble all the feature vectos into once vector for later downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def autoIndexer(df, lableCol, outputCol='assembled'):\n",
    "    stringTypes = [dtype[0] for dtype in df.dtypes if dtype[1] == 'string']\n",
    "    indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "    try:\n",
    "        indexedTypes.remove(lableCol+'Indexed')\n",
    "    except:\n",
    "        pass\n",
    "    indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed') for stringType in stringTypes]\n",
    "    oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "    ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "    assembler = VectorAssembler(inputCols=oheTypes, outputCol=outputCol)\n",
    "    pipeline = Pipeline(stages=[*indexers, ohe, assembler])    \n",
    "    indexed = pipeline.fit(df).transform(df)\n",
    "    return stringTypes, oheTypes, indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexed = train.select('education-num', 'education')\n",
    "\n",
    "indexer = StringIndexer(inputCol='education', outputCol='educationIndexed')\n",
    "indexed = indexer.fit(indexed).transform(indexed)\n",
    "ohe = OneHotEncoderEstimator(inputCols=['education-num',], outputCols=['education-numOHE',])\n",
    "indexed = ohe.fit(indexed).transform(indexed)\n",
    "\n",
    "# The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "# In general, small p-values (1% to 5%) would cause you to reject the null hypothesis. \n",
    "# This very large p-value (92.65%) means that the null hypothesis should not be rejected.\n",
    "testResult = ChiSquareTest.test(indexed, 'education-numOHE', 'educationIndexed')\n",
    "r = testResult.head()\n",
    "print(\"pValues: \" + str(r.pValues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accept the hypothesis that features are dependent. We will drop the 'education' feature since the info. is covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'workclass', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crosstabPercentage(train, 'hours-per-week', labelCol).orderBy('percentage-of->50K')\n",
    "df = df.orderBy('percentage-of->50K').withColumn('percentage-of->50K', \n",
    "                                    format_number(df['percentage-of->50K'], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indexedTypes, indexedDf = autoIndexer(train, labelCol)\n",
    "# The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "# In general, small p-values (1% to 5%) would cause you to reject the null hypothesis. \n",
    "# This very large p-value (92.65%) means that the null hypothesis should not be rejected.\n",
    "testResult = ChiSquareTest.test(indexedDf, 'assembled', 'classIndexed')\n",
    "r = testResult.head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2, featuresCol='assembled')\n",
    "model = kmeans.fit(indexedDf)\n",
    "indexedDf = model.transform(indexedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDf.select('prediction', 'classIndexed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stringTypes = [dtype[0] for dtype in train.dtypes if dtype[1] == 'string']\n",
    "indexedTypes = [stringType+'Indexed' for stringType in stringTypes]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=stringType, outputCol=stringType+'Indexed', handleInvalid='skip') \\\n",
    "            for stringType in stringTypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "oheTypes = [indexedType+'OneHotEncoded' for indexedType in indexedTypes]\n",
    "ohe = OneHotEncoderEstimator(inputCols=indexedTypes, outputCols=oheTypes)\n",
    "\n",
    "# Fix columns\n",
    "oheTypes.remove('classIndexedOneHotEncoded')\n",
    "cols = train.columns[:]\n",
    "for oheType in oheTypes:\n",
    "    cols.append(oheType)\n",
    "for stringType in stringTypes:\n",
    "    cols.remove(stringType)\n",
    "\n",
    "cols.remove('index')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol='assembled')\n",
    "classifier = GBTClassifier(featuresCol='assembled', labelCol='classIndexed')\n",
    "pipeline = Pipeline(stages=[*indexers, ohe, assembler, classifier])\n",
    "model = pipeline.fit(train)\n",
    "train = model.transform(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have class imbalance problem, that's why we will use area under ROC curve as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='classIndexed', metricName='areaUnderROC')\n",
    "metric = evaluator.evaluate(train)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.stages[-1]\n",
    "ohe = model.stages[-3]\n",
    "ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n",
    "           \"class\"]\n",
    "\n",
    "test = spark.read.csv('./adult.test.txt',\n",
    "                      inferSchema='true', \n",
    "                      ignoreLeadingWhiteSpace='true',\n",
    "                      ignoreTrailingWhiteSpace='true').toDF(*headers)\n",
    "test.select('class').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the class labels in the test dataset are different than in train - '>50K' and '>50K.'. So we have to remove the extrac dot from the class label, before evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "stripDot = udf(lambda s: s[:-1], StringType())\n",
    "\n",
    "test = test.withColumn('classTrailed', stripDot(test['class'])).drop('class').withColumnRenamed('classTrailed', 'class')\n",
    "test.select('class').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(test)\n",
    "metric = evaluator.evaluate(test)\n",
    "metric\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
